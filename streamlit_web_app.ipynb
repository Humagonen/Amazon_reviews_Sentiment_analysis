{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Humagonen/Amazon_reviews_Sentiment_analysis/blob/main/streamlit_web_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaQ-DGjVUsmm"
      },
      "source": [
        "# Streamlit Sentiment analysis web app"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fp78bMB74b1_",
        "outputId": "5092a9d0-7129-41db-fd04-58e988ded171"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATesuDKB4J62",
        "outputId": "415c68ac-7c31-4b49-ee9e-b105f57d7d6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\n",
            "changed 22 packages, and audited 23 packages in 1s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "1 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerability\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit tensorflow pandas numpy\n",
        "!npm install -g localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
        "import json\n",
        "\n",
        "# Load the model\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    return tf.keras.models.load_model('/content/drive/MyDrive/amazon reviews/review_amazon_sentiment5.h5')\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "# Load the tokenizer\n",
        "with open('/content/drive/MyDrive/amazon reviews/tokenizer.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "data_str = json.dumps(data)\n",
        "tokenizer = tokenizer_from_json(data_str)\n",
        "\n",
        "# Parameters\n",
        "num_words = 15000\n",
        "max_tokens = 166\n",
        "\n",
        "# Streamlit app\n",
        "st.title(\"Sentiment Analysis with LSTM\")\n",
        "\n",
        "# Section 1: Single Text Sentiment Analysis\n",
        "st.header(\"Single Text Sentiment Classification\")\n",
        "input_text = st.text_area(\"Enter the text to classify\", \"\")\n",
        "\n",
        "if st.button(\"Classify Sentiment\"):\n",
        "    if input_text:\n",
        "        # Tokenize and pad the input text\n",
        "        input_sequence = tokenizer.texts_to_sequences([input_text])\n",
        "        input_padded = pad_sequences(input_sequence, maxlen=max_tokens)\n",
        "\n",
        "        # Predict sentiment\n",
        "        prediction = model.predict(input_padded)[0][0]\n",
        "        sentiment = \"Negative\" if prediction > 0.5 else \"Positive\"\n",
        "\n",
        "        st.write(f\"Predicted Sentiment: **{sentiment}**\")\n",
        "        st.write(f\"Probability of being negative: {prediction:.2f}\")\n",
        "\n",
        "# Section 2: Batch Sentiment Analysis\n",
        "st.header(\"Batch Sentiment Classification\")\n",
        "uploaded_file = st.file_uploader(\"Upload a CSV file\", type=[\"csv\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Load the CSV\n",
        "    df = pd.read_csv(uploaded_file)\n",
        "\n",
        "    # Ensure the dataframe has the correct columns\n",
        "    if 'text' in df.columns:\n",
        "        st.write(\"Analyzing data...\")\n",
        "\n",
        "        # Tokenize and pad the text data (No need to fit the tokenizer again)\n",
        "        sequences = tokenizer.texts_to_sequences(df['text'].values)\n",
        "        padded_sequences = pad_sequences(sequences, maxlen=max_tokens)\n",
        "\n",
        "        # Predict sentiment\n",
        "        predictions = model.predict(padded_sequences)\n",
        "        df['sentiment_score'] = predictions\n",
        "        df['sentiment'] = df['sentiment_score'].apply(lambda x: \"Negative\" if x > 0.5 else \"Positive\")\n",
        "\n",
        "        # Display the dataframe\n",
        "        st.write(df.head())\n",
        "\n",
        "        # Download the resulting dataframe\n",
        "        csv = df.to_csv(index=False).encode('utf-8')\n",
        "        st.download_button(\n",
        "            label=\"Download sentiment analysis as CSV\",\n",
        "            data=csv,\n",
        "            file_name='sentiment.csv',\n",
        "            mime='text/csv',\n",
        "        )\n",
        "    else:\n",
        "        st.error(\"CSV must have a 'text' column.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSfnc-XGAQUl",
        "outputId": "505ed7ff-b21a-4f55-9c63-f44fd760bd87"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVWWEiuE4LC9",
        "outputId": "efafd346-3957-4f53-b736-1fc0768b0250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.125.133.211\n",
            "your url is: https://giant-pumas-relate.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com  # ip is your password"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuaFWixPUcPY"
      },
      "source": [
        "# Trying out model in notebook before streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_E3S4zJ-M6B"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "import json\n",
        "\n",
        "# Load the tokenizer JSON content as a string\n",
        "with open('/content/drive/MyDrive/amazon reviews/tokenizer.json', 'r') as f:\n",
        "    data = json.load(f)  # This gives you a dictionary\n",
        "\n",
        "# Convert the dictionary back to a JSON string\n",
        "data_str = json.dumps(data)\n",
        "\n",
        "# Use the string to load the tokenizer\n",
        "tokenizer = tokenizer_from_json(data_str)\n",
        "\n",
        "\n",
        "# load model\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/amazon reviews/review_amazon_sentiment.h5')\n",
        "\n",
        "\n",
        "# new data\n",
        "review1 = \"I hated this product, never buying it again!\"\n",
        "review2 = \"beautiful! fast shipping and a responsive seller\"\n",
        "review3 = \"garbage product, no one should sell such thing\"\n",
        "review4 = \"great price for a product like this, definitely buying it again\"\n",
        "\n",
        "reviews = [review1, review2, review3, review4]\n",
        "\n",
        "\n",
        "# apply tokenization\n",
        "num_words = 15000\n",
        "max_tokens = 162\n",
        "\n",
        "tokens = tokenizer.texts_to_sequences(reviews)\n",
        "tokens_pad = pad_sequences(tokens, maxlen=max_tokens)\n",
        "tokens_pad.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpQRdg1y_NwR",
        "outputId": "c8070224-000d-4186-fce9-ba43f5da4234"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 983ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# prediction\n",
        "\n",
        " (model.predict(tokens_pad) >0.5).astype(\"int\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xuaFWixPUcPY"
      ],
      "mount_file_id": "1iKkhOZMLjZKBJDhwXkdvI2RR-iPKFpTW",
      "authorship_tag": "ABX9TyPkpQjmZWEdAtSM6uJje0wq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}